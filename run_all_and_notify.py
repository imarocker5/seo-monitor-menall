import os, json, time, re
import feedparser, requests
from urllib.parse import urlparse
from datetime import datetime, timezone, timedelta
from email.utils import parsedate_to_datetime

# ===== ê³ ì • ì„¤ì • =====
BASE_FEED = "https://menall.kr/feed"           # menall.kr ê³ ì •
ANCHOR_TAG = "anchor"                          # íƒœê·¸ 'anchor'ë©´ ì•µì»¤
SITE_DOMAIN = "menall.kr"                      # ë‚´ë¶€ë§í¬ íŒì •

# ===== ì›¹í›…/ì˜µì…˜(ê¹ƒí—ˆë¸Œ Secretsë¡œ ì£¼ì… ê¶Œì¥) =====
SLACK_WEBHOOK   = os.getenv("SLACK_WEBHOOK", "")
DISCORD_WEBHOOK = os.getenv("DISCORD_WEBHOOK", "")

MIN_L_G2A = int(os.getenv("MIN_L_G2A", "1"))        # ì¼ë°˜â†’ì•µì»¤ ìµœì†Œ
MIN_A2C   = int(os.getenv("MIN_A2C", "6"))          # ì•µì»¤â†’ì¼ë°˜ ìµœì†Œ
ORPHAN_MAX_INTERNAL = int(os.getenv("ORPHAN_MAX_INTERNAL", "1"))
STALE_DAYS = int(os.getenv("STALE_DAYS", "90"))     # ì—…ë°ì´íŠ¸ í•„ìš” ì„ê³„(ì¼)

MAX_PAGES = int(os.getenv("MAX_PAGES", "200"))      # RSS í˜ì´ì§• ìƒí•œ
SLEEP_SEC = float(os.getenv("SLEEP_SEC", "0.4"))    # ìš”ì²­ ê°„ ë”œë ˆì´

# ===(ì„ íƒ) GPT ì œì•ˆ on/off & ëª¨ë¸===
ENABLE_GPT = os.getenv("ENABLE_GPT", "1") == "1"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# ===== ìœ í‹¸ =====
def is_internal(href: str) -> bool:
    if not href or href.startswith(("#","mailto:","tel:","javascript:")):
        return False
    try:
        u = urlparse(href)
        if not u.netloc:  # ìƒëŒ€ê²½ë¡œ
            return True
        return SITE_DOMAIN in u.netloc
    except:
        return False

def extract_links(html: str):
    if not html: return []
    return re.findall(r'href=[\'"]([^\'"]+)[\'"]', html, flags=re.I)

def days_since_iso_or_rfc822(date_str: str) -> int:
    if not date_str:
        return 9999
    try:
        dt = parsedate_to_datetime(date_str)
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return (datetime.now(timezone.utc) - dt).days
    except Exception:
        return 9999

def days_since(entry) -> int:
    tup = entry.get("updated_parsed")
    if tup:
        try:
            dt = datetime(*tup[:6], tzinfo=timezone.utc)
            return (datetime.now(timezone.utc) - dt).days
        except Exception:
            pass
    return days_since_iso_or_rfc822(entry.get("updated",""))

def now_kst_str():
    kst = datetime.now(timezone.utc) + timedelta(hours=9)
    return kst.strftime("%Y-%m-%d %H:%M")

def chunk_text(text, limit=1900):
    lines = text.splitlines(keepends=True)
    out, buf = [], ""
    for ln in lines:
        if len(buf) + len(ln) > limit:
            out.append(buf); buf = ""
        buf += ln
    if buf: out.append(buf)
    return out or ["(empty)"]

def send_message(text: str):
    sent_any = False
    # Slack
    if SLACK_WEBHOOK:
        try:
            r = requests.post(SLACK_WEBHOOK, json={"text": text}, timeout=15)
            print("[SLACK]", r.status_code, r.text[:200])
            if 200 <= r.status_code < 300: sent_any = True
        except Exception as e:
            print("[SLACK] error:", e)
    # Discord (204 ì •ìƒ, 2000ì ì œí•œ ëŒ€ì‘)
    if DISCORD_WEBHOOK:
        try:
            parts = chunk_text(text, limit=1900)
            for i, part in enumerate(parts, 1):
                prefix = f"[{i}/{len(parts)}] " if len(parts) > 1 else ""
                payload = {"content": prefix + part, "allowed_mentions": {"parse": []}}
                r = requests.post(DISCORD_WEBHOOK, json=payload, timeout=15)
                print("[DISCORD]", r.status_code, r.text[:200])
                if r.status_code == 204: sent_any = True
        except Exception as e:
            print("[DISCORD] error:", e)
    if not sent_any:
        print("[FALLBACK PRINT]\n" + text)

# (ì„ íƒ) ì˜¤ë˜ëœ ì•µì»¤ì— ëŒ€í•œ GPT 3ì¤„ ì œì•ˆ
def gpt_suggest_updates(title: str, html_snippet: str):
    if not (ENABLE_GPT and OPENAI_API_KEY):
        return None
    try:
        headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}
        payload = {
            "model": OPENAI_MODEL,
            "messages": [
                {"role":"system","content":"ë„ˆëŠ” SEO ì—ë””í„°ì´ì ì»¨í…ì¸  ë¦¬í”„ë ˆì‹œ ì¡°ì–¸ê°€ë‹¤."},
                {"role":"user","content": (
                    "ë‹¤ìŒ ì•µì»¤ ì½˜í…ì¸ ì˜ ì—…ë°ì´íŠ¸ í¬ì¸íŠ¸ 8ê°€ì§€ë¥¼ ê°„ê²°íˆ ì œì•ˆí•´ì¤˜.\n"
                    f"- ì œëª©: {title}\n"
                    f"- ë³¸ë¬¸ ì¼ë¶€(HTML í—ˆìš©): {html_snippet[:4000]}\n"
                    "í˜•ì‹:\n"
			"1) ìµœì‹  ë°ì´í„°/ì •í™•ì„± ë³´ê°•\n"
	"2) ë‚´ë¶€ë§í¬ ì¶”ê°€ ê¸°íšŒ\n"
	"3) FAQ/ì²´í¬ë¦¬ìŠ¤íŠ¸ ë³´ê°•\n"
	"4) ê²½ìŸ ì½˜í…ì¸  ëŒ€ë¹„ ì°¨ë³„í™” í¬ì¸íŠ¸\n"
	"5) ë¹„ì£¼ì–¼ ìš”ì†Œ ê°•í™”\n"
	"6) ì™¸ë¶€ ê¶Œìœ„ ë§í¬ ì¶”ê°€\n"
	"7) ê´€ë ¨ ê²€ìƒ‰ í‚¤ì›Œë“œ ë°˜ì˜\n"
	"8) ì»¤ë®¤ë‹ˆí‹°/ëŒ“ê¸€ ë°˜ì˜\n"
	"(ê³¼ì¥ ê¸ˆì§€, ì‚¬ì‹¤Â·ì‹ ì„ ë„Â·ì‚¬ìš©ì ê²½í—˜ ê´€ì )"
                )},
            ],
            "temperature": 0.4,
        }
        r = requests.post("https://api.openai.com/v1/chat/completions",
                          json=payload, headers=headers, timeout=30)
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"(GPT ì‹¤íŒ¨: {e})"

# ===== RSS ì „ì²´ ìˆ˜ì§‘ =====
def fetch_all_posts():
    seen, all_entries = set(), []
    def page_url(p): return f"{BASE_FEED}?paged={p}" if p > 1 else BASE_FEED
    for p in range(1, MAX_PAGES+1):
        url = page_url(p)
        print(f"ğŸ“¡ í˜ì´ì§€ {p}: {url}")
        feed = feedparser.parse(url)
        entries = feed.entries or []
        if not entries:
            print("   â– í•­ëª© ì—†ìŒ â†’ ì¢…ë£Œ"); break
        new_count = 0
        for e in entries:
            link = getattr(e, "link", "")
            if not link or link in seen: continue
            seen.add(link)
            cats = [t.term for t in getattr(e, "tags", [])] if hasattr(e, "tags") else []
            if isinstance(e.get("content"), list) and e.get("content"):
                content_value = e.get("content")[0].get("value", "")
            else:
                content_value = getattr(e, "summary", "")
            updated = getattr(e, "updated", "") or getattr(e, "published", "")
            updated_parsed = getattr(e, "updated_parsed", None) or getattr(e, "published_parsed", None)
            all_entries.append({
                "title": getattr(e, "title", ""),
                "link": link,
                "updated": updated,
                "updated_parsed": tuple(updated_parsed) if updated_parsed else None,
                "categories": cats,
                "content": content_value,
            })
            new_count += 1
        print(f"   â• ì‹ ê·œ {new_count}ê°œ (ì´ {len(all_entries)}ê°œ)")
        if new_count == 0: break
        time.sleep(SLEEP_SEC)
    return all_entries

# ===== ì ê²€ ë¡œì§ =====
def audit(entries):
    anchors, generals = [], []
    for e in entries:
        cats = [str(c).lower() for c in e.get("categories", [])]
        (anchors if ANCHOR_TAG in cats else generals).append(e)

    anchor_links  = set(a.get("link","").rstrip("/") for a in anchors)
    general_links = set(g.get("link","").rstrip("/") for g in generals)

    weak_g2a, weak_a2c, orphans, stale_anchors = [], [], [], []

    # ì¼ë°˜ê¸€: ì¼ë°˜â†’ì•µì»¤ / ê³ ì•„
    for g in generals:
        self_link = (g.get("link","") or "").rstrip("/")
        links = [h.rstrip("/") for h in extract_links(g.get("content","")) if is_internal(h)]
        links = [h for h in links if h != self_link]
        g2a = sum((h in anchor_links) for h in links)
        if g2a < MIN_L_G2A:
            weak_g2a.append((g.get("title",""), g.get("link",""), g2a))
        if len(links) <= ORPHAN_MAX_INTERNAL:
            orphans.append((g.get("title",""), g.get("link",""), len(links)))

    # ì•µì»¤ê¸€: ì•µì»¤â†’ì¼ë°˜ / ì˜¤ë˜ëœ ì•µì»¤
    for a in anchors:
        self_link = (a.get("link","") or "").rstrip("/")
        links = [h.rstrip("/") for h in extract_links(a.get("content","")) if is_internal(h)]
        links = [h for h in links if h != self_link]
        a2c = len({h for h in links if h in general_links})
        if a2c < MIN_A2C:
            weak_a2c.append((a.get("title",""), a.get("link",""), a2c))
        age = days_since(a)
        if age >= STALE_DAYS:
            stale_anchors.append((a.get("title",""), a.get("link",""), age, a.get("content","")[:2000]))

    return {
        "total": len(entries),
        "anchors": len(anchors),
        "generals": len(generals),
        "weak_g2a": weak_g2a,
        "weak_a2c": weak_a2c,
        "orphans": orphans,
        "stale_anchors": sorted(stale_anchors, key=lambda x: x[2], reverse=True)
    }

def main():
    entries = fetch_all_posts()
    result = audit(entries)

    lines = []
    lines.append(f"ğŸ›°ï¸ menall.kr ëª¨ë‹ˆí„°ë§ â€” {now_kst_str()} (KST)")
    lines.append(f"- ì´ {result['total']} / ì•µì»¤ {result['anchors']} / ì¼ë°˜ {result['generals']}")
    lines.append(f"- ê¸°ì¤€: ì¼ë°˜â†’ì•µì»¤â‰¥{MIN_L_G2A}, ì•µì»¤â†’ì¼ë°˜â‰¥{MIN_A2C}, ê³ ì•„â‰¤{ORPHAN_MAX_INTERNAL}, ì•µì»¤ê²½ê³¼â‰¥{STALE_DAYS}ì¼")
    lines.append("")

    # ì˜¤ë˜ëœ ì•µì»¤ + (ì„ íƒ) GPT ì œì•ˆ
    if result["stale_anchors"]:
        lines.append(f"âš ï¸ ì—…ë°ì´íŠ¸ í•„ìš” ì•µì»¤({len(result['stale_anchors'])}): (ê²½ê³¼ì¼â†“)")
        for t, link, age, snippet in result["stale_anchors"][:10]:
            lines.append(f"â€¢ {t} â€” {age}ì¼ ê²½ê³¼ â†’ {link}")
            if ENABLE_GPT and OPENAI_API_KEY:
                tip = gpt_suggest_updates(t, snippet)
                if tip: lines.append("  â”” ì—…ë°ì´íŠ¸ í¬ì¸íŠ¸:\n" + tip)
        lines.append("")

    if result["weak_g2a"]:
        lines.append(f"ğŸ”— ì¼ë°˜â†’ì•µì»¤ ë§í¬ ë¶€ì¡±({len(result['weak_g2a'])}):")
        for t, link, c in result["weak_g2a"][:20]:
            lines.append(f"â€¢ {t} ({c}) â†’ {link}")
        lines.append("")

    if result["weak_a2c"]:
        lines.append(f"ğŸ•¸ï¸ ì•µì»¤â†’ì¼ë°˜(í´ëŸ¬ìŠ¤í„°) ë§í¬ ë¶€ì¡±({len(result['weak_a2c'])}):")
        for t, link, c in result["weak_a2c"][:20]:
            lines.append(f"â€¢ {t} ({c}) â†’ {link}")
        lines.append("")

    if result["orphans"]:
        lines.append(f"ğŸ¥š ê³ ì•„ ìœ„í—˜ ì¼ë°˜ê¸€({len(result['orphans'])}): (ë‚´ë¶€ë§í¬ â‰¤ {ORPHAN_MAX_INTERNAL})")
        for t, link, c in result["orphans"][:20]:
            lines.append(f"â€¢ {t} (ë‚´ë¶€ë§í¬ {c}) â†’ {link}")
        lines.append("")

    if not (result["weak_g2a"] or result["weak_a2c"] or result["orphans"] or result["stale_anchors"]):
        lines.append("âœ… íŠ¹ì´ì‚¬í•­ ì—†ìŒ. êµ¬ì¡°/ë§í¬/ì‹ ì„ ë„ ì–‘í˜¸.")

    send_message("\n".join(lines))

if __name__ == "__main__":
    main()
